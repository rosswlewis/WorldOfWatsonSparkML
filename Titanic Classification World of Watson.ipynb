{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Machine Learning:  will YOU survive the Titanic?\n",
    "<img src='https://raw.githubusercontent.com/bradenrc/Spark_POT/master/Modules/MachineLearning/Classification/titanic.jpg' width=\"70%\" height=\"70%\"></img>\n",
    "#### With Spark, we can easily describe data and use it to make predictions.  We'll be using the famous Titanic data set from Kaggle (https://www.kaggle.com/c/titanic/data) and the machine learning package in Spark to do just that.\n",
    "## Access your data\n",
    "#### We have the titanic data on an instance of Object Storage, a cloud datat store for access and storage of unstructured data content.  We'll configure the connection here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_hadoop_config(name):\n",
    "    prefix = 'fs.swift.service.' + name\n",
    "    hconf = sc._jsc.hadoopConfiguration()\n",
    "    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n",
    "    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n",
    "    hconf.set(prefix + '.tenant', 'XXXXXXXX')\n",
    "    hconf.set(prefix + '.username', 'XXXXXXXX')\n",
    "    hconf.set(prefix + '.password', 'XXXXXXXX')\n",
    "    hconf.setInt(prefix + '.http.port', 8080)\n",
    "    hconf.set(prefix + '.region', 'dallas')\n",
    "    hconf.setBoolean(prefix + '.public', True)\n",
    "\n",
    "name = 'keystone'\n",
    "set_hadoop_config(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "#### Once we have the data, all of the processing is done in memory.  Here, we're formatting the data, removing columns, dropping rows with insufficient data, creating a DataFrame, and creating columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|   Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund Mr. Owen H...|  male|22.0|    1|    0|A/5 21171|   7.25|     |       S|\n",
      "|          2|       1|     1|Cumings Mrs. John...|female|38.0|    1|    0| PC 17599|71.2833|  C85|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext,Row\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "loadTitanicData = sqlContext.read.format(\"csv\").options(header=\"true\",inferSchema=\"true\").load(\"swift://XXXXXXXX.keystone/train.csv\")\n",
    "loadTitanicData.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----+-----+-----+-------+--------+--------+\n",
      "|Pclass|   Sex| Age|SibSp|Parch|   Fare|Embarked|Survived|\n",
      "+------+------+----+-----+-----+-------+--------+--------+\n",
      "|     3|  male|22.0|    1|    0|   7.25|       S|     0.0|\n",
      "|     1|female|38.0|    1|    0|71.2833|       C|     1.0|\n",
      "+------+------+----+-----+-----+-------+--------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[Pclass: int, Sex: string, Age: double, SibSp: int, Parch: int, Fare: double, Embarked: string, Survived: double]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadTitanicData = loadTitanicData.drop(\"PassengerId\").drop(\"Name\").drop(\"Ticket\").drop(\"Cabin\").dropna(subset=[\"Age\", \"Embarked\"])\n",
    "loadTitanicData = loadTitanicData.withColumn(\"SurvivedTemp\", loadTitanicData[\"Survived\"].cast(\"double\")).drop(\"Survived\").withColumnRenamed(\"SurvivedTemp\",\"Survived\")\n",
    "loadTitanicData.show(2)\n",
    "loadTitanicData.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your Spark.ML pipeline\n",
    "#### String Indexer and One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "SexIndexer = StringIndexer().setInputCol(\"Sex\").setOutputCol(\"SexIndex\").setHandleInvalid(\"skip\")\n",
    "EmbarkedIndexer = StringIndexer().setInputCol(\"Embarked\").setOutputCol(\"EmbarkedIndex\").setHandleInvalid(\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "SexEncoder = OneHotEncoder(inputCol=\"SexIndex\", outputCol=\"SexFeatures\")\n",
    "EmbarkedEncoder = OneHotEncoder(inputCol=\"EmbarkedIndex\",outputCol=\"EmbarkedFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bucketizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-------------------+------------------+------------------+\n",
      "|summary|            Pclass|               Age|             SibSp|              Parch|              Fare|          Survived|\n",
      "+-------+------------------+------------------+------------------+-------------------+------------------+------------------+\n",
      "|  count|               714|               714|               714|                714|               714|               714|\n",
      "|   mean|2.2366946778711485| 29.69911764705882|0.5126050420168067|0.43137254901960786|34.694514005602215|0.4061624649859944|\n",
      "| stddev| 0.838249862698379|14.526497332334042|0.9297834541221924| 0.8532893658062201|52.918929502543556|0.4914598643353705|\n",
      "|    min|                 1|              0.42|                 0|                  0|               0.0|               0.0|\n",
      "|    max|                 3|              80.0|                 5|                  6|          512.3292|               1.0|\n",
      "+-------+------------------+------------------+------------------+-------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loadTitanicData.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "AgeSplits = [-float(\"inf\"), 4.0, 12.0, 18.0, 35.0, 60.0, 80.0, float(\"inf\")]\n",
    "FareSplits = [-float(\"inf\"), 20.0, 50.0, 100.0, float(\"inf\")]\n",
    "\n",
    "AgeBucketizer = Bucketizer(splits=AgeSplits, inputCol=\"Age\", outputCol=\"AgeBuckets\")\n",
    "FareBucketizer = Bucketizer(splits=FareSplits, inputCol=\"Fare\", outputCol=\"FareBuckets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "Assembler = VectorAssembler(inputCols=[\"SexFeatures\", \"EmbarkedFeatures\", \"AgeBuckets\", \"FareBuckets\", \"SibSp\", \"Pclass\", \"Parch\"],outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "Normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regresssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.2, elasticNetParam=0.8, featuresCol=\"normFeatures\", labelCol=\"Survived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All added into the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "pipeline = Pipeline(stages=[SexIndexer, EmbarkedIndexer, SexEncoder, EmbarkedEncoder, AgeBucketizer, FareBucketizer, Assembler, Normalizer, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a testing data set and training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[train, test] = loadTitanicData.randomSplit([.75, .25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a model from the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict whether or not a passenger will survive the titanic using unobserved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/src/spark160master/spark/python/pyspark/ml/classification.py:207: UserWarning: weights is deprecated. Use coefficients instead.\n",
      "  warnings.warn(\"weights is deprecated. Use coefficients instead.\")\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Pclass=3, Sex=u'male', Age=20.0, SibSp=0, Parch=0, Fare=8.05, Embarked=u'S', Survived=0.0, SexIndex=0.0, EmbarkedIndex=0.0, SexFeatures=0.0, EmbarkedFeatures=SparseVector(3, {0: 1.0}), AgeBuckets=3.0, FareBuckets=0.0, features=SparseVector(9, {1: 1.0, 4: 3.0, 7: 3.0}), normFeatures=SparseVector(9, {1: 0.2294, 4: 0.6882, 7: 0.6882}), rawPrediction=DenseVector([0.7168, -0.7168]), probability=DenseVector([0.6719, 0.3281]), prediction=0.0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+--------+------+-----+-----+--------+----------+\n",
      "| Sex| Age|   Fare|Embarked|Pclass|Parch|SibSp|Survived|prediction|\n",
      "+----+----+-------+--------+------+-----+-----+--------+----------+\n",
      "|male|20.0|   8.05|       S|     3|    0|    0|     0.0|       0.0|\n",
      "|male|39.0| 31.275|       S|     3|    5|    1|     0.0|       0.0|\n",
      "|male|19.0|  263.0|       S|     1|    2|    3|     0.0|       0.0|\n",
      "|male|66.0|   10.5|       S|     2|    0|    0|     0.0|       0.0|\n",
      "|male|65.0|61.9792|       C|     1|    1|    0|     0.0|       0.0|\n",
      "+----+----+-------+--------+------+-----+-----+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+----+-------+--------+------+-----+-----+--------+----------+\n",
      "|   Sex| Age|   Fare|Embarked|Pclass|Parch|SibSp|Survived|prediction|\n",
      "+------+----+-------+--------+------+-----+-----+--------+----------+\n",
      "|female|55.0|   16.0|       S|     2|    0|    0|     1.0|       1.0|\n",
      "|female|38.0|31.3875|       S|     3|    5|    1|     1.0|       0.0|\n",
      "|female|29.0|   26.0|       S|     2|    0|    1|     1.0|       1.0|\n",
      "|  male|0.83|   29.0|       S|     2|    2|    0|     1.0|       0.0|\n",
      "|female|30.0| 12.475|       S|     3|    0|    0|     1.0|       1.0|\n",
      "+------+----+-------+--------+------+-----+-----+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.filter(\"Survived = 0.0\").select(\"Sex\", \"Age\", \"Fare\", \"Embarked\", \"Pclass\", \"Parch\", \"SibSp\", \"Survived\", \"prediction\").show(5)\n",
    "predictions.filter(\"Survived = 1.0\").select(\"Sex\", \"Age\", \"Fare\", \"Embarked\", \"Pclass\", \"Parch\", \"SibSp\", \"Survived\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and tune your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.797305318139\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Survived\", metricName=\"areaUnderROC\")\n",
    "print evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = ParamGridBuilder().baseOn({lr.labelCol: 'Survived'}).baseOn([lr.predictionCol, 'prediction'])\\\n",
    "    .addGrid(lr.elasticNetParam,[0.0,0.4,0.8]).addGrid(lr.maxIter, [2, 10, 20]).addGrid(lr.regParam, [0.0,0.1,0.2,0.3])\\\n",
    "    .build()\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
    "\n",
    "cvModel = cv.fit(train)\n",
    "newPredictions = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve for non-tuned model = 0.797305318139\n",
      "Area under the ROC curve for best fitted model = 0.884021842355\n",
      "Improvement = 10.876200402%\n"
     ]
    }
   ],
   "source": [
    "print \"Area under the ROC curve for non-tuned model = \" + str(evaluator.evaluate(predictions))\n",
    "print \"Area under the ROC curve for best fitted model = \" + str(evaluator.evaluate(newPredictions))\n",
    "print \"Improvement = \" + str((evaluator.evaluate(newPredictions) - evaluator.evaluate(predictions)) *100 / evaluator.evaluate(predictions)) + \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Will YOU survive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+--------+------+-----+-----+----------+\n",
      "| Sex| Age|Fare|Embarked|Pclass|SibSp|Parch|prediction|\n",
      "+----+----+----+--------+------+-----+-----+----------+\n",
      "|male|27.0|15.0|       C|     2|    1|    1|       0.0|\n",
      "+----+----+----+--------+------+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField(\"Sex\", StringType(), True),StructField(\"Age\", DoubleType(), True),\\\n",
    "                    StructField(\"Fare\", DoubleType(), True),StructField(\"Embarked\", StringType(), True),\\\n",
    "                    StructField(\"Pclass\", IntegerType(), True),StructField(\"SibSp\", IntegerType(), True),\\\n",
    "                    StructField(\"Parch\", IntegerType(), True)])\n",
    "me = sc.parallelize([(\"male\",28.0,15.0,\"C\",2,1,1)])\n",
    "\n",
    "PredictionFeatures = sqlContext.createDataFrame(me,schema)\n",
    "SurvivedOrNotPrediction = cvModel.transform(PredictionFeatures)\n",
    "SurvivedOrNotPrediction.select(\"Sex\", \"Age\", \"Fare\", \"Embarked\", \"Pclass\", \"SibSp\", \"Parch\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 1.6",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}