{"metadata": {"language_info": {"name": "scala"}, "kernelspec": {"display_name": "Scala 2.10", "name": "scala", "language": "scala"}}, "nbformat_minor": 0, "cells": [{"metadata": {}, "source": "# Reddit Comment Clustering Using TF-IDF\n<img src=\"https://raw.githubusercontent.com/rosswlewis/RedditComments/master/reddit_log-100011890-large.jpg\" align=\"left\" width=\"35%\">", "cell_type": "markdown"}, {"metadata": {}, "source": "# Let's add a postgres driver and import all of the spark libraries we need", "cell_type": "markdown"}, {"metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Starting download from https://jdbc.postgresql.org/download/postgresql-9.4.1207.jre7.jar\nFinished download of postgresql-9.4.1207.jre7.jar\n"}], "source": "%Addjar -f https://jdbc.postgresql.org/download/postgresql-9.4.1207.jre7.jar", "cell_type": "code", "execution_count": 1}, {"metadata": {"collapsed": false}, "outputs": [], "source": "import org.apache.spark.sql._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.ml.feature.{RegexTokenizer, StopWordsRemover, HashingTF, IDF, Normalizer}\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.clustering.KMeans\nimport org.apache.spark.ml.PipelineModel\n\nval sqlContext = new SQLContext(sc)", "cell_type": "code", "execution_count": 1}, {"metadata": {}, "source": "# Here, we read in the data from ", "cell_type": "markdown"}, {"metadata": {"collapsed": false}, "outputs": [], "source": "val uname = \"readonly\"\nval pword = \"watson\"\nval dbUrl = \n\"jdbc:postgresql://haproxy635.sl-us-dal-9-portal.2.dblayer.com:10635/compose?user=\"+uname+\"&password=\"+pword\nval table = \"reddit\"\nval comments = sqlContext.read.format(\"jdbc\").options(\n  Map(\"url\" -> dbUrl,\n  \"dbtable\" -> table)).load()", "cell_type": "code", "execution_count": 5}, {"metadata": {}, "source": "# To clean the data, we only use comments that haven't been deleted, are over 100 characters long, and we remove punctuation", "cell_type": "markdown"}, {"metadata": {"collapsed": false}, "outputs": [], "source": "val commentLower = comments.filter(\"length(body) >= 100\").\n    select(lower(comments(\"body\")).alias(\"lowerText\")).distinct()", "cell_type": "code", "execution_count": 6}, {"metadata": {"collapsed": false}, "outputs": [], "source": "val tokenizer = new RegexTokenizer().\n    setInputCol(\"lowerText\").\n    setOutputCol(\"words\").\n    setPattern(\"\\\\W+\")", "cell_type": "code", "execution_count": 5}, {"metadata": {}, "source": "# There are a lot of words that we don't want to have an affect on clusters.  Here, we add a stop words remover to our pipeline. ", "cell_type": "markdown"}, {"metadata": {"collapsed": false}, "outputs": [{"data": {"text/plain": "stopWords_f68dff9a0d7c"}, "metadata": {}, "execution_count": 6, "output_type": "execute_result"}], "source": "val additionalWords = Array(\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\n                            \"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\",\n                            \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\n                            \"oh\",\"wow\",\"stuff\",\"thank\",\"isn\",\"don\",\"didn\",\"people\",\n                            \"thing\",\"ve\",\"time\",\"know\",\"think\")\nval remover = new StopWordsRemover().setInputCol(\"words\").setOutputCol(\"noStopWords\")\nval currentStopWords = remover.getStopWords\nval allStopWords = currentStopWords ++ additionalWords\nremover.setStopWords(allStopWords)", "cell_type": "code", "execution_count": 6}, {"metadata": {}, "source": "# Add a column which hashes each individual word to an integer, and mark the frequency of that word in the comment", "cell_type": "markdown"}, {"metadata": {"collapsed": false}, "outputs": [], "source": "val hashingTF = new HashingTF().setInputCol(\"noStopWords\").setOutputCol(\"hashingTF\").setNumFeatures(25000)", "cell_type": "code", "execution_count": 7}, {"metadata": {}, "source": "# Add a column which looks at all words in all comments and ranks the importance", "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "source": "val idf = new IDF().setInputCol(\"hashingTF\").setOutputCol(\"idf\")", "cell_type": "code", "execution_count": 8}, {"metadata": {}, "source": "# Normalize the output", "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "source": "val normalizer = new Normalizer().\n    setInputCol(\"idf\").\n    setOutputCol(\"features\")", "cell_type": "code", "execution_count": 9}, {"metadata": {}, "source": "# Based on the important words of different comments, create a pipeline to cluster the comments by topic", "cell_type": "markdown"}, {"metadata": {"collapsed": false}, "outputs": [], "source": "val kmeans = new KMeans().\n    setFeaturesCol(\"features\").\n    setPredictionCol(\"prediction\").\n    setK(250)\nval pipeline = new Pipeline().\n    setStages(Array(tokenizer, remover, hashingTF, idf, normalizer, kmeans))\nval model = pipeline.fit(commentLower)", "cell_type": "code", "execution_count": 10}, {"metadata": {"collapsed": false}, "outputs": [], "source": "val predictionsDF = model.transform(commentLower)", "cell_type": "code", "execution_count": 7}, {"metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "1546943\n"}], "source": "println(predictionsDF.count())", "cell_type": "code", "execution_count": 1}, {"metadata": {}, "source": "<img src=\"https://raw.githubusercontent.com/rosswlewis/RedditComments/master/TFIDF-FIG-01.JPG\" align=\"left\">", "cell_type": "markdown"}, {"metadata": {}, "source": "# Save the model and labeled data for future use", "cell_type": "markdown"}, {"metadata": {"scrolled": true, "collapsed": false}, "outputs": [], "source": "var credentials = scala.collection.mutable.HashMap[String, String](\n  \"auth_url\"->\"https://identity.open.softlayer.com\",\n  \"project\"->\"XXXXXXXX\",\n  \"project_id\"->\"XXXXXXXX\",\n  \"region\"->\"dallas\",\n  \"user_id\"->\"XXXXXXXX\",\n  \"domain_id\"->\"XXXXXXXX\",\n  \"domain_name\"->\"XXXXXXXX\",\n  \"username\"->\"XXXXXXXX\",\n  \"password\"->\"\"\"XXXXXXXX\"\"\",\n  \"container\"->\"XXXXXXXX\",\n  \"tenantId\"->\"XXXXXXXX\",\n  \"name\"->\"keystone\"\n)\ndef setHadoopConfig(name: String, tenant: String, url: String, \n                    username: String, password: String, region: String) = {\n    sc.hadoopConfiguration.set(f\"fs.swift.service.$name.auth.url\",url+\"/v3/auth/tokens\")\n    sc.hadoopConfiguration.set(f\"fs.swift.service.$name.auth.endpoint.prefix\",\"endpoints\")\n    sc.hadoopConfiguration.set(f\"fs.swift.service.$name.tenant\",tenant)\n    sc.hadoopConfiguration.set(f\"fs.swift.service.$name.username\",username)\n    sc.hadoopConfiguration.set(f\"fs.swift.service.$name.password\",password)\n    sc.hadoopConfiguration.setInt(f\"fs.swift.service.$name.http.port\",8080)\n    sc.hadoopConfiguration.set(f\"fs.swift.service.$name.region\",region)\n    sc.hadoopConfiguration.setBoolean(f\"fs.swift.service.$name.public\",true)\n}\nsetHadoopConfig(credentials(\"name\"),credentials(\"project_id\"),credentials(\"auth_url\"),\n                          credentials(\"user_id\"),credentials(\"password\"),credentials(\"region\"))\n//sc.parallelize(Seq(model), 1).saveAsObjectFile(\"swift://XXXXXXXX.keystone/model\")\n//predictionsDF.select(\"noStopWords\",\"lowerText\",\"prediction\").write.parquet(\"swift://XXXXXXXX.keystone/commentClusters\")", "cell_type": "code", "execution_count": null}, {"metadata": {"collapsed": false}, "outputs": [], "source": "val model = sc.objectFile[PipelineModel](\"swift://XXXXXXXX.keystone/model\").first()", "cell_type": "code", "execution_count": 3}, {"metadata": {"collapsed": false}, "outputs": [{"data": {"text/plain": "pipeline_8ac1f844e0a4"}, "metadata": {}, "execution_count": 4, "output_type": "execute_result"}], "source": "model", "cell_type": "code", "execution_count": 4}, {"metadata": {}, "source": "# Create a new comment and assign a cluster to it", "cell_type": "markdown"}, {"metadata": {"collapsed": false}, "outputs": [], "source": "import org.apache.spark.sql.Row;\nimport org.apache.spark.sql.types.{StructType,StructField,StringType};\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.types._\nimport sqlContext.implicits._\n\nval userInput = \"With my free time, I like to play video games, drum, and do big data analytics.\"\ncase class comment(lowerText: String)\nval commentDataFrame = sc.parallelize(Seq(comment(userInput))).toDF()\n\nval curCom = model.transform(commentDataFrame)", "cell_type": "code", "execution_count": 4}, {"metadata": {"collapsed": false}, "outputs": [{"data": {"text/plain": "Array([With my free time, I like to play video games, drum, and do big data analytics.,185])"}, "metadata": {}, "execution_count": 5, "output_type": "execute_result"}], "source": "curCom.select(\"lowerText\",\"prediction\").collect()", "cell_type": "code", "execution_count": 5}, {"metadata": {"collapsed": true}, "outputs": [], "source": "", "cell_type": "code", "execution_count": null}], "nbformat": 4}